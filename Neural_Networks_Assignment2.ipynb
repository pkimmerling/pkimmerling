{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tested-meditation",
   "metadata": {},
   "source": [
    "Importing the data from the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "spectacular-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import csv\n",
    "from scipy.optimize import fmin_bfgs,minimize\n",
    "\n",
    "#data specific numbers\n",
    "class1start = 0\n",
    "class2start = 59\n",
    "class3start = 130\n",
    "\n",
    "#global parameters\n",
    "regParam = 0.03 #use in cost function\n",
    "nSPerClass = 40 #number of samples per class\n",
    "nS = 3*nSPerClass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-mirror",
   "metadata": {},
   "source": [
    "Stuff from Assignment \\#1 which we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "included-reason",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LReLU, optional to leave as ReLU by passing esp=0 \n",
    "def lrelu(x,b=1,esp=0.01):\n",
    "    y=[]\n",
    "    for xi in x:\n",
    "        if xi<0:\n",
    "            y.append(esp*xi)\n",
    "        else:\n",
    "            y.append(b*xi)\n",
    "    return array(y)\n",
    "def lrelu_prime(x,b=1,esp=0.01):\n",
    "    y=[]\n",
    "    for xi in x:\n",
    "        if xi<0:\n",
    "            y.append(esp)\n",
    "        else:\n",
    "            y.append(b)\n",
    "    return array(y)\n",
    "#x is input vector, W is weight estimation (use random entries between 0 and 1 for first estimate, call N somewhere)\n",
    "def feed_forward(x,W):\n",
    "    W0 = array(W[:k*m]).reshape((k,m))\n",
    "    b0 = W[k*m:k*m+k]\n",
    "    W1 = array(W[(m+1)*k:(m+1)*k+n*k]).reshape((n,k))\n",
    "    b1 = W[(m+1)*k+n*k:]\n",
    "    z1 = dot(W0,x)+b0\n",
    "    z2 = dot(W1,a1(z1))+b1\n",
    "    return a2(z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-fluid",
   "metadata": {},
   "source": [
    "Begin Assignment \\#2 here. Much is borrowed from your code but used my own terms as I wrote it out to make sure I understood. I am going to use the binary encoding rather than one-hot so I need the ReLU and Logistic activation functions, and I'm using an ensemble cost function rather than the other method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "coral-springfield",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------The rest of the functions we should need are in here----------------------------------\n",
    "#many of which have been hybrid-ed with Dr. COo\n",
    "#accepts a column vector argument, returns mean and l2 norm of vector\n",
    "def normalize(col): \n",
    "    l2norm = sqrt(sum((col-mean(col))*(col-mean(col)))/len(col))\n",
    "    return mean(col),l2norm\n",
    "\n",
    "#accepts no arguments, returns array of mean values for data & array of norm values\n",
    "def normalize_cols(inMat):\n",
    "    means=[]\n",
    "    norms=[]\n",
    "    for i in range(m):\n",
    "        mn,nrm=normalize(inMat[:,i])\n",
    "        means.append(mn)\n",
    "        norms.append(nrm)\n",
    "    meanValues=array(means)\n",
    "    normValues=array(norms)\n",
    "    return meanValues,normValues\n",
    "\n",
    "#accepts a matrix, returns the normalized matrix\n",
    "def normalizeByTrain(inMat):\n",
    "    meanVals,normVals = normalize_cols(inMat)\n",
    "    normedMat=inMat*0.0\n",
    "    for k in range(len(inMat[:,0])):\n",
    "        normedMat[k]=(inMat[k]-meanVals)/normVals\n",
    "    return normedMat\n",
    "\n",
    "#accepts input matrices X and Y and vector W\n",
    "def ensemble_cost(W,X,Y):\n",
    "    C = 0.0\n",
    "    for i in range(len(X[0,:])):\n",
    "        x=X[i,:]\n",
    "        y=Y[i,:]\n",
    "        C += 0.5*(dot(feed_forward(x,W)-y,feed_forward(x,W)-y)+regParam*0.5*linalg.norm(W)**2)\n",
    "    return C\n",
    "\n",
    "#Dr. Cooper's logistic function & it's derivative below + his other functions b/c I think they'll work more reliably\n",
    "def S(X):\n",
    "    X = X*(abs(X)<10)+10.*(X>=10)-10.*(X<=-10)\n",
    "    return 1./(1+exp(-X))\n",
    "\n",
    "def SPrime(X):\n",
    "    X = X*(abs(X)<10)+10.*(X>=10)-10.*(X<=-10)\n",
    "    ex = exp(-X)\n",
    "    return ex/((1.+ex)*(1.+ex))\n",
    "\n",
    "def relu(x,epsilon=0.0):\n",
    "    return x*(x>0)+epsilon*x*(x<=0)\n",
    "\n",
    "def reluPrime(x,epsilon=0.0):\n",
    "    return (x>0)+epsilon*(x<=0)\n",
    "\n",
    "def selu(x,lamda=1,alpha=1):\n",
    "    return lamda*( x*(x>0)+alpha*(exp(x)-1)*(x<=0))\n",
    "\n",
    "def seluPrime(x,lamda=1,alpha=1):\n",
    "    return lamda*((x>0)+alpha*exp(x)*(x<=0))\n",
    "\n",
    "#accepts input vector x, output vector out, and big W vector, returns a (regularized) gradient for one row\n",
    "def gradient(x,out,W):#<----borrowed\n",
    "    W0 = array(W[:nW0]).reshape((k,m))\n",
    "    W1 = array(W[nW0+k:-n]).reshape((n,k))\n",
    "    grad = W*0.0\n",
    "    # Get a(z_2)\n",
    "    z1 = dot(W0,x)+W[nW0:nW0+k]\n",
    "    z2 = dot(W1,a1(z1))+W[-n:]\n",
    "    forward = a2(z2)\n",
    "    err = forward-out\n",
    "    # gradient for b_1 bias weights\n",
    "    finalLayerDeriv = a2p(z2)*err\n",
    "    grad[-n:] = finalLayerDeriv+0.\n",
    "    # gradient for W_1 weights\n",
    "    grad[(nW0+k):-n] = outer(finalLayerDeriv,a1(z1)).flatten()\n",
    "    # gradient for b_0 bias weights\n",
    "    firstLayerDeriv = finalLayerDeriv.dot(W1)*a1p(z1)\n",
    "    grad[nW0:(nW0+k)] = firstLayerDeriv+0.\n",
    "    # gradient for W_0 weights\n",
    "    grad[:nW0] = outer(firstLayerDeriv,x).flatten()\n",
    "    return grad+regParam*W\n",
    "\n",
    "#accepts vector W, input matrix X and output matrix Y\n",
    "def gradSum(W,X,Y):\n",
    "    grad = zeros(nVar)\n",
    "    for i in range(len(X[:,0])): #X[:,0] is a slice of all the rows in the first column\n",
    "        grad += gradient(X[i,:],Y[i,:],W)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-cable",
   "metadata": {},
   "source": [
    "The code which will actually produce the neural network structure now is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lightweight-bacteria",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 178\n",
      "Number of data vectors: 120\n",
      "Number of data items: 13\n",
      "Number of hidden cells: 11\n",
      "[[1.423e+01 1.710e+00 2.430e+00 ... 1.040e+00 3.920e+00 1.065e+03]\n",
      " [1.320e+01 1.780e+00 2.140e+00 ... 1.050e+00 3.400e+00 1.050e+03]\n",
      " [1.316e+01 2.360e+00 2.670e+00 ... 1.030e+00 3.170e+00 1.185e+03]\n",
      " ...\n",
      " [1.282e+01 3.370e+00 2.300e+00 ... 7.200e-01 1.750e+00 6.850e+02]\n",
      " [1.358e+01 2.580e+00 2.690e+00 ... 7.400e-01 1.800e+00 7.500e+02]\n",
      " [1.340e+01 4.600e+00 2.860e+00 ... 6.700e-01 1.920e+00 6.300e+02]]\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 1.106804\n",
      "         Iterations: 33\n",
      "         Function evaluations: 158\n",
      "         Gradient evaluations: 146\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 1.106804\n",
      "         Iterations: 0\n",
      "         Function evaluations: 74\n",
      "         Gradient evaluations: 62\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 1.106804\n",
      "         Iterations: 0\n",
      "         Function evaluations: 74\n",
      "         Gradient evaluations: 62\n"
     ]
    }
   ],
   "source": [
    "file = csv.reader(open(\"uciWineLocation.csv\",\"r\"))\n",
    "rowIndex = 0\n",
    "inData = []\n",
    "outData = [] #will contain truth values for each of the three classes, i.e. 1 == [true false false], used for one-hot?\n",
    "outSData = [] #will contain the numerical values corresponding to each class\n",
    "testBin = 3*[0]\n",
    "\n",
    "for row in file:\n",
    "    for ri in range(len(row)):\n",
    "        row[ri] = float(row[ri])\n",
    "    inData.append(row[1:])\n",
    "    for i in range(1,4):\n",
    "        #testBin[i-1] = (row[0]==float(i))#this line writes it as one-hot\n",
    "        testBin[i-1] = () #this line writes it as binary \n",
    "    outData.append(testBin.copy())#takes the encoded outdata to actually run through the \n",
    "    outSData.append(row[0]) #what to compare to later, the numbers 1,2,3\n",
    "#Peeling off the training data into vectors\n",
    "trainX = array(inData[:nSPerClass]+inData[class2start:class2start+nSPerClass]+inData[class3start:class3start+nSPerClass]) #rows are input data\n",
    "trainY = array(outData[:nSPerClass]+outData[class2start:class2start+nSPerClass]+outData[class3start:class3start+nSPerClass]).astype(float) #rows are output data\n",
    "trainSY = array(outSData[:nSPerClass]+outSData[class2start:class2start+nSPerClass]+outSData[class3start:class3start+nSPerClass]).astype(float) #thing to check results with\n",
    "m = len(trainX[0]) #m is the number of inputs, i.e. number of columns in first row\n",
    "n = 3 #n is the number of outputs desired\n",
    "k = int(2*m/3+n) #k is the number of hidden layers\n",
    "W = random.rand((m + 1)*k+(k + 1)*n) #our first estimate of our W\n",
    "print('Number of samples: {}'.format(len(inData)))\n",
    "print('Number of data vectors: {}'.format(nS))\n",
    "print('Number of data items: {}'.format(m))\n",
    "print('Number of hidden cells: {}'.format(k))\n",
    "print(trainX)\n",
    "nW0 = m*k\n",
    "nW1 = k*n\n",
    "nVar = nW0+nW1+k+n\n",
    "\n",
    "#normalize the vector\n",
    "trainX = normalizeByTrain(trainX)\n",
    "#compute cost, choosing a1 to be relu and a2 to be logistic\n",
    "a1 = lambda x: lrelu(x)\n",
    "a1p = lambda x: lrelu_prime(x)\n",
    "a2 = S\n",
    "a2p = SPrime\n",
    "#call training function, I'm going to use bfgs_min to solve just because it seems best\n",
    "newW = W\n",
    "for i in range(3):\n",
    "    newW = fmin_bfgs(ensemble_cost,newW,fprime=gradSum,args=(trainX,trainY))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "virtual-halloween",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.80939128 0.18656869 0.17530175]\n",
      " [0.65460699 0.26358702 0.29845248]\n",
      " [0.91495864 0.08701188 0.09697915]\n",
      " [0.56258268 0.27345968 0.37792653]\n",
      " [0.71496832 0.27494569 0.21656329]\n",
      " [0.77803318 0.12087301 0.22841051]\n",
      " [0.88841764 0.11201389 0.11856756]\n",
      " [0.83678222 0.16116658 0.15628108]\n",
      " [0.79565293 0.1967     0.18733489]\n",
      " [0.87358694 0.12569529 0.13039767]\n",
      " [0.83934054 0.15927141 0.15449574]\n",
      " [0.89617832 0.10513751 0.111984  ]\n",
      " [0.92074196 0.0819204  0.09163942]\n",
      " [0.89031855 0.10880022 0.11859584]\n",
      " [0.82174751 0.17360181 0.1682023 ]\n",
      " [0.79746972 0.19648413 0.18431558]\n",
      " [0.85686117 0.14166585 0.14229268]\n",
      " [0.86689247 0.13117344 0.13616713]\n",
      " [0.89194619 0.1089183  0.11557424]\n",
      " [0.16900111 0.82930829 0.14355574]\n",
      " [0.16604188 0.82860664 0.17576412]\n",
      " [0.1847707  0.80012744 0.20082077]\n",
      " [0.27612288 0.62417712 0.33950354]\n",
      " [0.1393643  0.84950617 0.16361465]\n",
      " [0.20831198 0.7813655  0.21099208]\n",
      " [0.13923103 0.64609902 0.36223605]\n",
      " [0.20974979 0.7781577  0.21512367]\n",
      " [0.14195666 0.5525652  0.44943458]\n",
      " [0.15788154 0.83606838 0.17149183]\n",
      " [0.3676759  0.62478283 0.17866782]\n",
      " [0.20756201 0.78250768 0.2116083 ]\n",
      " [0.20431381 0.75511067 0.23812943]\n",
      " [0.23263226 0.51071899 0.4556238 ]\n",
      " [0.15838527 0.83490817 0.17277035]\n",
      " [0.21015601 0.76603365 0.2266153 ]\n",
      " [0.05097823 0.95142184 0.06908768]\n",
      " [0.14176288 0.85357909 0.15755656]\n",
      " [0.15842292 0.83518617 0.17207888]\n",
      " [0.13520141 0.26594753 0.72606604]\n",
      " [0.19912422 0.78201658 0.21486109]\n",
      " [0.32070085 0.6624994  0.24991191]\n",
      " [0.53214846 0.45203331 0.2277308 ]\n",
      " [0.18211863 0.49055297 0.49018886]\n",
      " [0.27381698 0.42087317 0.5309864 ]\n",
      " [0.30882614 0.67413376 0.25578149]\n",
      " [0.20590985 0.783      0.21147605]\n",
      " [0.25931565 0.68758052 0.2871827 ]\n",
      " [0.12015537 0.63306459 0.38097147]\n",
      " [0.1627324  0.81335595 0.19284   ]\n",
      " [0.16452184 0.37043118 0.61315524]\n",
      " [0.0982611  0.10432737 0.89244936]\n",
      " [0.05726084 0.05833574 0.94443638]\n",
      " [0.07324124 0.07211312 0.92769339]\n",
      " [0.05070289 0.04911861 0.95345715]\n",
      " [0.07890699 0.07965664 0.91941693]\n",
      " [0.05633341 0.05504165 0.94673724]\n",
      " [0.07942686 0.07927827 0.91949642]\n",
      " [0.04682698 0.04503687 0.95774069]]\n",
      "[[1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "#the test data\n",
    "testX = array(inData[nSPerClass:class2start]+inData[class2start+nSPerClass:class3start]+inData[class3start+nSPerClass:]) #rows are the rest of the input data\n",
    "testY = array(outData[nSPerClass:class2start]+outData[class2start+nSPerClass:class3start]+outData[class3start+nSPerClass:]).astype(float) #rows are rest of output data\n",
    "testSY = array(outSData[nSPerClass:class2start]+outSData[class2start+nSPerClass:class3start]+outSData[class3start+nSPerClass:]).astype(float) #thing to check results with    \n",
    "binTest = \n",
    "testX = normalizeByTrain(testX)\n",
    "#now feed forward again but on test data with theoretically more accurate W\n",
    "pred=[]\n",
    "for i in arange(len(testX[:,0])): #i iterates over number of rows in training data\n",
    "    x=testX[i,:] #x grabs the i-th row\n",
    "    result=feed_forward(x,newW)\n",
    "    pred.append(result)\n",
    "\n",
    "print(array(pred))\n",
    "print((array(pred)+0.5).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-skill",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
