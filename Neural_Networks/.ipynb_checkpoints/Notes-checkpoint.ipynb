{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "covered-shuttle",
   "metadata": {},
   "source": [
    "January 18th"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-shield",
   "metadata": {},
   "source": [
    "The output of a neural network is a composition of functions of the weights, and the neural network problem is to choose the weights to approximate desired output data. So it is a minimization problem, so take derivative and set equal to zero (think first in one variable). Write it out as a Taylor series and omit higher order terms, get Newton's method. It is a linear problem so we make iterations and hope it converges -- converges when initial guess is close enough. \n",
    "\n",
    "To solve nonlinear problems: \n",
    "1. Get a linear approx and iterate it via Newton's method -- in higher dimensions, G'(x_0) is a Jacobian and it becomes a matrix problem.\n",
    "2. If we're trying to find $g(w)-y=0$, find the l2 norm of that instead. Choose descent direction, and take a small step, iterate wishfully.\n",
    "Today we'll solve linear problems directly and next week we'll solve descent problem:\n",
    "1. LU -- lower upper (choose 1's on main diagonal in lower matrix); used by linalg.solve()\n",
    "2. Cholesky -- LU in disguise: when A is symmetrix and positive definite, we see that LU = A = A^T = U^TL^T, get A=LU=L(DL^T), get ~L=LD^{1/2}. Preserves symmetry which LU does not do.\n",
    "--> Both are very sensitive, kind bois, make a lot of errors which propogate forward and amplify.\n",
    "3. QR --Q is unitary and R is rank-triangular (upper triangular) -- think Gram-Schmidt. Doesn't have to be square.\n",
    "--> If problem is badly conditioned, QR doesn't compensate for it but it can handle it better, doesn't propogate as much.\n",
    "4. SVD --singular values: U is mxm unitary, V is nxn unitary, and S is the mxn with singular values on diagonal, if A is real/symmetric, singular values = eigenvalues. Unlimited power! A need not be square. Also, we can throw away small singular values, improve condition number (called regularization).\n",
    "\n",
    "The condition nunmber of a matrix A is the largest singular value divided by the smallest. This multiplier hits the errors and if the condition number is too big, we fall apart. 1-1000 is probably a fine condition number, above 5000, 10000 is going to lose precision IF it's able to solve it, and more than 10^14, it is junk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-holocaust",
   "metadata": {},
   "source": [
    "January 25th "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-dutch",
   "metadata": {},
   "source": [
    "We will likely load numpy, matpotlib, and scikit-learn.\n",
    "There are different Python interpreters, such as CPython, Pypy-JIT, Cython. Cooper uses CPython with the Numba package for JIT commands.\n",
    "Python is an interpreted language, which means you write it once and the interpreter handles different platforms, and it handles packages. It is also easier, but it is slower because we run and compile at the same time. It is also bigger, takes more space and probably can't work with super large arrays.\n",
    "Python rewrites code file as bytecode (.py file ---> .pyc file).\n",
    "\n",
    "There are several ways to import Numpy: \n",
    "    1. import numpy: x = numpy.linspace(0,1,101)\n",
    "    2. import numpy as np: x = np.linspace(0,1,101)\n",
    "    3. from numpy import \\*: x = linspace(0,1,101)\n",
    "Sympy has namespace conflicts with numpy, so we probably import sympy as sy or something.\n",
    "\n",
    "x = linspace(0,1,101)\n",
    "g = sin(x)\\*\\*2\n",
    "from matplotlib.pyplot import \\*\n",
    "plot (x,g,'g')\n",
    "show()\n",
    "\n",
    "To make a partition, either use linspace or arange().\n",
    "\n",
    "Python does all oprtations elementwise so if we need to do vector operations, we use numpy functions.\n",
    "\n",
    "range() is better than arange() for running iterators in loops now that we're in Python 3\n",
    "\n",
    "Equating array names now identifies starting locations of arrays like C does..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "def fibon(Nterms=10):\n",
    "    f0 = 1\n",
    "    f1 = 1\n",
    "    k = 0\n",
    "    while k<Nterms:\n",
    "        yield f0\n",
    "        f0,f1 = f1,f0+f1\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-theory",
   "metadata": {},
   "source": [
    "Feburary 1st "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-composition",
   "metadata": {},
   "source": [
    "Lots of slicing rules. Can index using arrays so long as they are the same size. The dimensional indices can be used if we impose a tuple as the outermost structure. Can also use negative indices, fun. Colon notation only works in indices though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "declared-astronomy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1. 52.  0.]\n",
      " [ 0. -1. 52.]\n",
      " [ 0.  0. -1.]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import *\n",
    "dia = arange(3)\n",
    "A = zeros((3,3,))\n",
    "A[dia,dia] = -1\n",
    "ind = ([0,1],[1,2]) \n",
    "A[ind] = 52\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-offense",
   "metadata": {},
   "source": [
    "The linalg sublibrary uses BLAS and LAPack to provide functionality. Matrix-matrix, matrix-vector and vector inner products can all be done using dot(,). It will be smart about the dimensions and throw a red flag if it doesn't. It can go both ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imported-spread",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51. 51. -1.]\n",
      "[51. 51. -1.]\n",
      "[-1. 51. 51.]\n",
      "[-1. 51. 51.]\n",
      "[51. 51. -1.]\n"
     ]
    }
   ],
   "source": [
    "x = array([1,1,1])\n",
    "y = dot(A,x)\n",
    "print(y)\n",
    "yAlt = A.dot(x) # would prefer to do this because it can make use of sparse storage for A\n",
    "print(yAlt)\n",
    "print(dot(x,A))\n",
    "print(x.dot(A))\n",
    "print(x.dot(A.T)) #transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-semiconductor",
   "metadata": {},
   "source": [
    "Numpy provides ways to solve, but it doesn't have a function for LU (Lower/Uppper) decomposition!\n",
    "\n",
    "y = A.dot(x)\n",
    "newX = linalg.solve(A,y)\n",
    "\n",
    "But he does have one for Cholesky.\n",
    "\n",
    "C = linalg.cholesky(A.T.dot(A))\n",
    "\n",
    "If you really want LU, scipy has one:\n",
    "import scipy.linalg\n",
    "P,L,U = scipy.linalg(A)\n",
    "\n",
    "Numpy does have the common orthogonal decompositions:\n",
    "\n",
    "Q,R = linalg.qr(A), gives both unitary matrices as output in that order. More numerically stable but generally don't prefer it over LU. Also can give SVD:\n",
    "\n",
    "U,S,Vt = linalg.svd(A); S is always diagonal so he just puts the singular values out as a single vector/list thingy.\n",
    "\n",
    "He has a norm function, it is buried in linalg package: linalg.norm or linalg.matrix_norm maybe?\n",
    "\n",
    "When reading in data, it's a pain:\n",
    "\n",
    "import csv\n",
    "def read_csv(filename):\n",
    "    theInput = []\n",
    "    data = csv.reader(open(filename,'r'))\n",
    "    for row in data:\n",
    "        theInput.append(row)\n",
    "    return theInput\n",
    "\n",
    "mydata = read_csv('covidvaccine.dat')\n",
    "\n",
    "But more commonly we need to separate the data into floating point arrays:\n",
    "\n",
    "state = []\n",
    "percentInfection = []\n",
    "percentMask = []\n",
    "data = data = csv.reader(open('maskWearing.csv','r'))\n",
    "for row in data:\n",
    "    state.append(row[0])\n",
    "    percentInfection.append(row[1])\n",
    "    percentMask.append(row[2])\n",
    "percentInfection = array(percentInfection).astype(float)\n",
    "percentMask = array(percentMask).astype(float)\n",
    "\n",
    "Object oriented programming. Create self-contained memory blocks together with instructions to maintain it. We can then create instances of that structure, assigned to variables. It organizes code for reuse. In industry, OOP is what everyone does. It also collates memory for one logical object and so it's most useful for very large programs (5,000 to 50,000 lines of code). We must be aware of the cache.\n",
    "\n",
    "\n",
    "Every class needs a constructor - a function that is called whenever an instance of the class is created.\n",
    "\n",
    "import scipy.stats as stats\n",
    "class regressionLine:\n",
    "    def __init__(self,xVals,yVals):\n",
    "    self.xVals = xVals[:] #this makes \n",
    "    self.yVals = yVals[:]\n",
    "    self.slope,self.intercept,self.rValue,self.pValue,self.stdErr = stats.linregress(xVals,yVals)\n",
    "    \n",
    "After that, we can make two instances of the class as e.g.:\n",
    "import KlamathKenoMonthly as kkm\n",
    "y1930 = regressionLine(kkm.data[:12,4],kk.data[:12,5])\n",
    "\n",
    "A method is just a function but it's a function that belongs to a certain class. Go back up and add this function def to the class:\n",
    "\n",
    "def plotRegression(self,xLabl='',yLabl='',titl=''):\n",
    "    plot(self.xVals,self.yVals,'bp')\n",
    "    plot(self.xVals,self.slope*self.xVals + self.intercept, 'r')\n",
    "    stuff\n",
    "    stuff I missed *\n",
    "    \n",
    "y1930.plotregression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-kennedy",
   "metadata": {},
   "source": [
    "February 8th"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-eugene",
   "metadata": {},
   "source": [
    "Solve the linear system Ax=b by using additive decomposition of $A=L+D+U$, L is strictly lower part, U is strictly upper part, D is diagonal.\n",
    "\n",
    "Jacobi: $Dx_k=y-(L+U)x_{k-1}$\n",
    "\n",
    "Gauss-Seidel: $(L+D)x_k = y-Ux_{k-1}$\n",
    "\n",
    "Successive OverRelaxation: $(D+\\omega L)x_k = \\omega y-(\\omega U+(\\omega-1)D]x_{k-1}$\n",
    "\n",
    "Can use gradient descent to reach solution which minimizes $||Ax-y||_2^2$ but this can go fast in one direction and then slow down if we have a bad manafold. \n",
    "\n",
    "If A is symmetric and positive definite, we can use conjugate gradient: suppose we have a set of vectors which are orthogonal (conjugate) in the given inner product. Then we can forma  basis for the space and write the solution x to Ax=y. It is related to the spectral structure of A too, since the eigenvalues are positive and real and eigenvectors themselves orthogonal. But that's expensive so we will first step in a descent direction and then use a Gram-Schmidt procedure to choose the rest of the p vectors. So choose $p_0=(y-Ax_0)$ then define $x_1=x_0+\\alpha_0p_0$ and minimize along that line, so any remaining residual is orthogonal to that subspace AND then we iterate with Gram Schmidt.\n",
    "\n",
    "MINRES does conjugate gradient when it can and regularizes when it can't; GMRES does \n",
    "\n",
    "You can pre-condition the matrix so that things go more quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-newfoundland",
   "metadata": {},
   "source": [
    "February 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-accuracy",
   "metadata": {},
   "source": [
    "Ready to start typing next time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
