{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Identifying Wine Heritage By Chemical Composition</h1>\n",
    "\n",
    "1. Title of Database: Wine recognition data\n",
    "        Updated Sept 21, 1998 by C.Blake : Added attribute information\n",
    "\n",
    "2. Sources:\n",
    "   (a) Forina, M. et al, PARVUS - An Extendible Package for Data\n",
    "       Exploration, Classification and Correlation. Institute of Pharmaceutical\n",
    "       and Food Analysis and Technologies, Via Brigata Salerno,\n",
    "       16147 Genoa, Italy.\n",
    "\n",
    "   (b) Stefan Aeberhard, email: stefan@coral.cs.jcu.edu.au\n",
    "   (c) July 1991\n",
    "\n",
    "\n",
    "Note that we use global variables throughout this.  A better way to do this for production code would be to make a class, but this is just a small demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 178\n",
      "Number of data vectors: 120\n",
      "Number of data items: 13\n",
      "Number of hidden cells: 11\n"
     ]
    }
   ],
   "source": [
    "from numpy import *\n",
    "from matplotlib.pyplot import *\n",
    "from scipy.optimize import fmin_bfgs,minimize\n",
    "import csv\n",
    "import time as t\n",
    "\n",
    "\n",
    "# Globals to control parameters\n",
    "regParam = 0.03\n",
    "nSPerClass = 40\n",
    "nS = 3*nSPerClass\n",
    "\n",
    "class1start = 0\n",
    "class2start = 59\n",
    "class3start = 130\n",
    "\n",
    "data = csv.reader(open('uciWineLocation.csv','r'))\n",
    "rowInd = 0\n",
    "inData = []\n",
    "outData = []\n",
    "outSData = []\n",
    "testBin = 3*[0]\n",
    "\n",
    "# Read the data as strings\n",
    "# Just continue with global variables - it is a small program\n",
    "for row in data:\n",
    "    for ri in range(len(row)):\n",
    "        row[ri] = float(row[ri])\n",
    "    inData.append(row[1:])\n",
    "    for ri in range(1,4):\n",
    "        testBin[ri-1] = (row[0]==float(ri))\n",
    "    outData.append(testBin.copy())\n",
    "    outSData.append(row[0])\n",
    "    \n",
    "# Parcel into training data, leaving some out\n",
    "trainX = array(inData[:nSPerClass]+inData[class2start:class2start+nSPerClass]+inData[class3start:class3start+nSPerClass])\n",
    "trainY = array(outData[:nSPerClass]+outData[class2start:class2start+nSPerClass]+outData[class3start:class3start+nSPerClass]).astype(float)\n",
    "trainSY = array(outSData[:nSPerClass]+outSData[class2start:class2start+nSPerClass]+outSData[class3start:class3start+nSPerClass]).astype(float)\n",
    "nInput = len(trainX[0])\n",
    "nOutput = 3\n",
    "nHidden = int(2*nInput/3+nOutput)\n",
    "\n",
    "print('Number of samples: {}'.format(len(inData)))\n",
    "print('Number of data vectors: {}'.format(nS))\n",
    "print('Number of data items: {}'.format(nInput))\n",
    "print('Number of hidden cells: {}'.format(nHidden))\n",
    "\n",
    "nW0 = nInput*nHidden\n",
    "nW1 = nHidden*nOutput\n",
    "nVar = nW0+nW1+nHidden+nOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some normalization stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(col):\n",
    "    # Normalize a column of input data\n",
    "    numSamp = float(len(col))\n",
    "    mu = sum(col)/numSamp\n",
    "    l2norm = sqrt(sum((col-mu)*(col-mu))/numSamp)\n",
    "    return mu,l2norm\n",
    "\n",
    "# Find normalization coefficients for each column\n",
    "def normalizationConst():\n",
    "    muVals = []\n",
    "    normVals = []\n",
    "    for i in range(nInput):\n",
    "        mu,nrm = normalization(trainX[:,i])\n",
    "        muVals.append(mu)\n",
    "        normVals.append(nrm)\n",
    "    muVals = array(muVals)\n",
    "    normVals = array(normVals)\n",
    "    return muVals,normVals\n",
    "\n",
    "# Normalize the rows\n",
    "def normalizeByTrain(inputMatrix):\n",
    "    muVals, normVals = normalizationConst()\n",
    "    nSamp = len(inputMatrix[:,0])\n",
    "    outMatrix = inputMatrix*0.0\n",
    "    for k in range(nSamp):\n",
    "        outMatrix[k] = (inputMatrix[k]-muVals)/normVals\n",
    "    return outMatrix\n",
    "\n",
    "trainX = normalizeByTrain(trainX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize weights, feed forward and cost/loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalW = random.rand(nVar)\n",
    "\n",
    "output = eye(nOutput)\n",
    "\n",
    "def feedForward(x,W):\n",
    "    W0 = array(W[:nW0]).reshape(nHidden,nInput)\n",
    "    W1 = array(W[(nW0+nHidden):-nOutput]).reshape(nOutput,nHidden)\n",
    "    hiddenResult = a1(W0.dot(x)+W[nW0:(nW0+nHidden)])\n",
    "    z2 = W1.dot(hiddenResult)+W[-nOutput:]\n",
    "    return a2(z2)\n",
    "\n",
    "def cost(forward,output,W):\n",
    "    return 0.5*( dot(forward-output,forward-output)+regParam*0.5*linalg.norm(W)**2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the activations and a gradient for a one-hidden-layer network.  Note that we can pick activations by setting the a1,a1p, a2, and a2p variables.\n",
    "\n",
    "Note also that we just used a sum of costs and grads for our ensemble cost and grad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def S(X):\n",
    "    X = X*(abs(X)<10)+10.*(X>=10)-10.*(X<=-10)\n",
    "    return 1./(1+exp(-X))\n",
    "\n",
    "def SPrime(X):\n",
    "    X = X*(abs(X)<10)+10.*(X>=10)-10.*(X<=-10)\n",
    "    ex = exp(-X)\n",
    "    return ex/((1.+ex)*(1.+ex))\n",
    "\n",
    "def relu(x,epsilon=0.0):\n",
    "    return x*(x>0)+epsilon*x*(x<=0)\n",
    "\n",
    "def reluPrime(x,epsilon=0.0):\n",
    "    return (x>0)+epsilon*(x<=0)\n",
    "\n",
    "def selu(x,lamda=1,alpha=1):\n",
    "    return lamda*( x*(x>0)+alpha*(exp(x)-1)*(x<=0))\n",
    "\n",
    "def seluPrime(x,lamda=1,alpha=1):\n",
    "    return lamda*((x>0)+alpha*exp(x)*(x<=0))\n",
    "\n",
    "a1 = lambda x: relu(x,epsilon=.1)\n",
    "a1p = lambda x: reluPrime(x,epsilon=.1)\n",
    "a2 = S\n",
    "a2p = SPrime\n",
    "\n",
    "def gradient(x,out,W):\n",
    "    W0 = array(W[:nW0]).reshape(nHidden,nInput)\n",
    "    W1 = array(W[nW0+nHidden:-nOutput]).reshape(nOutput,nHidden)\n",
    "    grad = W*0.0\n",
    "    # Get a(z_2)\n",
    "    z1 = dot(W0,x)+W[nW0:nW0+nHidden]\n",
    "    z2 = dot(W1,a1(z1))+W[-nOutput:]\n",
    "    forward = a2(z2)\n",
    "    err = forward-out\n",
    "    # gradient for b_1 bias weights\n",
    "    finalLayerDeriv = a2p(z2)*err\n",
    "    grad[-nOutput:] = finalLayerDeriv+0.\n",
    "    # gradient for W_1 weights\n",
    "    grad[(nW0+nHidden):-nOutput] = outer(finalLayerDeriv,a1(z1)).flatten()\n",
    "    # gradient for b_0 bias weights\n",
    "    firstLayerDeriv = finalLayerDeriv.dot(W1)*a1p(z1)\n",
    "    grad[nW0:(nW0+nHidden)] = firstLayerDeriv+0.\n",
    "    # gradient for W_0 weights\n",
    "    grad[:nW0] = outer(firstLayerDeriv,x).flatten()\n",
    "    return grad+regParam*W\n",
    "\n",
    "def gradSum(W,X,Y):\n",
    "    nSamp = len(X[:,0])\n",
    "    grad = zeros(nVar)\n",
    "    for i in range(nSamp):\n",
    "        grad += gradient(X[i,:],Y[i,:],W)\n",
    "    return grad\n",
    "\n",
    "def forwardTest(W,x,y):\n",
    "    return cost(feedForward(x,W),y,W)\n",
    "\n",
    "def forwardTestAll(W,X,Y):\n",
    "    tot = 0.\n",
    "    nSamp = len(X[:,0])\n",
    "    for i in range(nSamp):\n",
    "        xSamp = X[i,:]\n",
    "        ySamp = Y[i,:]\n",
    "        tot += forwardTest(W,xSamp,ySamp)\n",
    "    return tot    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
