{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tested-meditation",
   "metadata": {},
   "source": [
    "Importing the data from the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "spectacular-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import csv\n",
    "from scipy.optimize import fmin_bfgs,minimize\n",
    "\n",
    "#data specific numbers\n",
    "class1start = 0\n",
    "class2start = 59\n",
    "class3start = 130\n",
    "\n",
    "#global parameters\n",
    "regParam = 0.03 #use in cost function\n",
    "nSPerClass = 40 #number of samples per class\n",
    "nS = 3*nSPerClass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-mirror",
   "metadata": {},
   "source": [
    "Stuff from Assignment \\#1 which we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "included-reason",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LReLU, optional to leave as ReLU by passing esp=0 \n",
    "def lrelu(x,b=1,esp=0.01):\n",
    "    y=[]\n",
    "    for xi in x:\n",
    "        if xi<0:\n",
    "            y.append(esp*xi)\n",
    "        else:\n",
    "            y.append(b*xi)\n",
    "    return array(y)\n",
    "def lrelu_prime(x,b=1,esp=0.01):\n",
    "    y=[]\n",
    "    for xi in x:\n",
    "        if xi<0:\n",
    "            y.append(esp)\n",
    "        else:\n",
    "            y.append(b)\n",
    "    return array(y)\n",
    "#x is input vector, W is weight estimation (use random entries between 0 and 1 for first estimate, call N somewhere)\n",
    "def feed_forward(x,W):\n",
    "    W0 = array(W[:k*m]).reshape((k,m))\n",
    "    b0 = W[k*m:k*m+k]\n",
    "    W1 = array(W[(m+1)*k:(m+1)*k+n*k]).reshape((n,k))\n",
    "    b1 = W[(m+1)*k+n*k:]\n",
    "    z1 = dot(W0,x)+b0\n",
    "    z2 = dot(W1,a1(z1))+b1\n",
    "    return a2(z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-fluid",
   "metadata": {},
   "source": [
    "Begin Assignment \\#2 here. Much is borrowed from your code but used my own terms as I wrote it out to make sure I understood. I am going to use the binary encoding rather than one-hot so I need the ReLU and Logistic activation functions, and I'm using an ensemble cost function rather than the other method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "coral-springfield",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------The rest of the functions we should need are in here----------------------------------\n",
    "#many of which have been hybrid-ed with Dr. COo\n",
    "#accepts a column vector argument, returns mean and l2 norm of vector\n",
    "def normalize(col): \n",
    "    l2norm = sqrt(sum((col-mean(col))*(col-mean(col)))/len(col))\n",
    "    return mean(col),l2norm\n",
    "\n",
    "#accepts no arguments, returns array of mean values for data & array of norm values\n",
    "def normalize_cols(inMat):\n",
    "    means=[]\n",
    "    norms=[]\n",
    "    for i in range(m):\n",
    "        mn,nrm=normalize(inMat[:,i])\n",
    "        means.append(mn)\n",
    "        norms.append(nrm)\n",
    "    meanValues=array(means)\n",
    "    normValues=array(norms)\n",
    "    return meanValues,normValues\n",
    "\n",
    "#accepts a matrix, returns the normalized matrix\n",
    "def normalizeByTrain(inMat):\n",
    "    meanVals,normVals = normalize_cols(inMat)\n",
    "    normedMat=inMat*0.0\n",
    "    for k in range(len(inMat[:,0])):\n",
    "        normedMat[k]=(inMat[k]-meanVals)/normVals\n",
    "    return normedMat\n",
    "\n",
    "#accepts input matrices X and Y and vector W\n",
    "def ensemble_cost(W,X,Y):\n",
    "    C = 0.0\n",
    "    for i in range(len(X[0,:])):\n",
    "        x=X[i,:]\n",
    "        y=Y[i,:]\n",
    "        C += 0.5*(dot(feed_forward(x,W)-y,feed_forward(x,W)-y)+regParam*0.5*linalg.norm(W)**2)\n",
    "    return C\n",
    "\n",
    "#Dr. Cooper's logistic function & it's derivative below + his other functions b/c I think they'll work more reliably\n",
    "def S(X):\n",
    "    X = X*(abs(X)<10)+10.*(X>=10)-10.*(X<=-10)\n",
    "    return 1./(1+exp(-X))\n",
    "\n",
    "def SPrime(X):\n",
    "    X = X*(abs(X)<10)+10.*(X>=10)-10.*(X<=-10)\n",
    "    ex = exp(-X)\n",
    "    return ex/((1.+ex)*(1.+ex))\n",
    "\n",
    "def relu(x,epsilon=0.0):\n",
    "    return x*(x>0)+epsilon*x*(x<=0)\n",
    "\n",
    "def reluPrime(x,epsilon=0.0):\n",
    "    return (x>0)+epsilon*(x<=0)\n",
    "\n",
    "def selu(x,lamda=1,alpha=1):\n",
    "    return lamda*( x*(x>0)+alpha*(exp(x)-1)*(x<=0))\n",
    "\n",
    "def seluPrime(x,lamda=1,alpha=1):\n",
    "    return lamda*((x>0)+alpha*exp(x)*(x<=0))\n",
    "\n",
    "#accepts input vector x, output vector out, and big W vector, returns a (regularized) gradient for one row\n",
    "def gradient(x,out,W):#<----borrowed\n",
    "    W0 = array(W[:nW0]).reshape((k,m))\n",
    "    W1 = array(W[nW0+k:-n]).reshape((n,k))\n",
    "    grad = W*0.0\n",
    "    # Get a(z_2)\n",
    "    z1 = dot(W0,x)+W[nW0:nW0+k]\n",
    "    z2 = dot(W1,a1(z1))+W[-n:]\n",
    "    forward = a2(z2)\n",
    "    err = forward-out\n",
    "    # gradient for b_1 bias weights\n",
    "    finalLayerDeriv = a2p(z2)*err\n",
    "    grad[-n:] = finalLayerDeriv+0.\n",
    "    # gradient for W_1 weights\n",
    "    grad[(nW0+k):-n] = outer(finalLayerDeriv,a1(z1)).flatten()\n",
    "    # gradient for b_0 bias weights\n",
    "    firstLayerDeriv = finalLayerDeriv.dot(W1)*a1p(z1)\n",
    "    grad[nW0:(nW0+k)] = firstLayerDeriv+0.\n",
    "    # gradient for W_0 weights\n",
    "    grad[:nW0] = outer(firstLayerDeriv,x).flatten()\n",
    "    return grad+regParam*W\n",
    "\n",
    "#accepts vector W, input matrix X and output matrix Y\n",
    "def gradSum(W,X,Y):\n",
    "    grad = zeros(nVar)\n",
    "    for i in range(len(X[:,0])): #X[:,0] is a slice of all the rows in the first column\n",
    "        grad += gradient(X[i,:],Y[i,:],W)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-cable",
   "metadata": {},
   "source": [
    "The code which will actually produce the neural network structure now is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "lightweight-bacteria",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 178\n",
      "Number of data vectors: 120\n",
      "Number of data items: 13\n",
      "Number of hidden cells: 11\n",
      "[[1.423e+01 1.710e+00 2.430e+00 ... 1.040e+00 3.920e+00 1.065e+03]\n",
      " [1.320e+01 1.780e+00 2.140e+00 ... 1.050e+00 3.400e+00 1.050e+03]\n",
      " [1.316e+01 2.360e+00 2.670e+00 ... 1.030e+00 3.170e+00 1.185e+03]\n",
      " ...\n",
      " [1.282e+01 3.370e+00 2.300e+00 ... 7.200e-01 1.750e+00 6.850e+02]\n",
      " [1.358e+01 2.580e+00 2.690e+00 ... 7.400e-01 1.800e+00 7.500e+02]\n",
      " [1.340e+01 4.600e+00 2.860e+00 ... 6.700e-01 1.920e+00 6.300e+02]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,) (785,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-253-43f72459bf7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mnewW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mnewW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfmin_bfgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensemble_cost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnewW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfprime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgradSum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/miniconda3/envs/sage/lib/python3.9/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfmin_bfgs\u001b[0;34m(f, x0, fprime, args, gtol, norm, epsilon, maxiter, full_output, disp, retall, callback)\u001b[0m\n\u001b[1;32m   1041\u001b[0m             'return_all': retall}\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_minimize_bfgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfprime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfull_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/sage/lib/python3.9/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_minimize_bfgs\u001b[0;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0mmaxiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m     sf = _prepare_scalar_function(fun, x0, jac, args=args, epsilon=eps,\n\u001b[0m\u001b[1;32m   1102\u001b[0m                                   finite_diff_rel_step=finite_diff_rel_step)\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/sage/lib/python3.9/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;31m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;31m# calculation reduces overall function evaluations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m     sf = ScalarFunction(fun, x0, args, grad, hess,\n\u001b[0m\u001b[1;32m    262\u001b[0m                         finite_diff_rel_step, bounds, epsilon=epsilon)\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/sage/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# Gradient evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/sage/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/sage/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/sage/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnfev\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-252-614a3859cbe8>\u001b[0m in \u001b[0;36mensemble_cost\u001b[0;34m(W, X, Y)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mC\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mregParam\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-251-e662122a173a>\u001b[0m in \u001b[0;36mfeed_forward\u001b[0;34m(x, W)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mz2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,) (785,) "
     ]
    }
   ],
   "source": [
    "W = random.rand((m + 1)*k+(k + 1)*n) #our first estimate of our W\n",
    "file = csv.reader(open(\"uciWineLocation.csv\",\"r\"))\n",
    "rowIndex = 0\n",
    "inData = []\n",
    "outData = [] #will contain truth values for each of the three classes, i.e. 1 == [true false false], used for one-hot?\n",
    "outSData = [] #will contain the numerical values corresponding to each class\n",
    "testBin = 3*[0]\n",
    "\n",
    "for row in file:\n",
    "    for ri in range(len(row)):\n",
    "        row[ri] = float(row[ri])\n",
    "    inData.append(row[1:])\n",
    "    for i in range(1,4):\n",
    "        testBin[i-1] = (row[0]==float(i))\n",
    "    outData.append(testBin.copy())\n",
    "    outSData.append(row[0])\n",
    "#Peeling off the training data into vectors\n",
    "trainX = array(inData[:nSPerClass]+inData[class2start:class2start+nSPerClass]+inData[class3start:class3start+nSPerClass]) #rows are input data\n",
    "trainY = array(outData[:nSPerClass]+outData[class2start:class2start+nSPerClass]+outData[class3start:class3start+nSPerClass]).astype(float) #rows are output data\n",
    "trainSY = array(outSData[:nSPerClass]+outSData[class2start:class2start+nSPerClass]+outSData[class3start:class3start+nSPerClass]).astype(float) #thing to check results with\n",
    "m = len(trainX[0]) #m is the number of inputs, i.e. number of columns in first row\n",
    "n = 3 #n is the number of outputs desired\n",
    "k = int(2*m/3+n) #k is the number of hidden layers\n",
    "\n",
    "print('Number of samples: {}'.format(len(inData)))\n",
    "print('Number of data vectors: {}'.format(nS))\n",
    "print('Number of data items: {}'.format(m))\n",
    "print('Number of hidden cells: {}'.format(k))\n",
    "print(trainX)\n",
    "nW0 = m*k\n",
    "nW1 = k*n\n",
    "nVar = nW0+nW1+k+n\n",
    "#normalize the vector\n",
    "trainX = normalizeByTrain(trainX)\n",
    "#compute cost, choosing a1 to be relu and a2 to be logistic\n",
    "a1 = lambda x: lrelu(x)\n",
    "a1p = lambda x: lrelu_prime(x)\n",
    "a2 = S\n",
    "a2p = SPrime\n",
    "#call training function, I'm going to use bfgs_min to solve just because it seems best\n",
    "newW = W\n",
    "for i in range(3):\n",
    "    newW = fmin_bfgs(ensemble_cost,newW,fprime=gradSum,args=(trainX,trainY))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "virtual-halloween",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.81176223 0.24260783 0.14702792]\n",
      " [0.61730927 0.35854115 0.25437501]\n",
      " [0.94325982 0.10363375 0.07546286]\n",
      " [0.50559346 0.3765421  0.34014953]\n",
      " [0.68728017 0.34743919 0.20095348]\n",
      " [0.82594456 0.13675251 0.19427579]\n",
      " [0.91576338 0.13662475 0.09390304]\n",
      " [0.84526808 0.21099379 0.13154894]\n",
      " [0.80253271 0.24851751 0.15223529]\n",
      " [0.90501503 0.14782832 0.10030689]\n",
      " [0.83242546 0.22405083 0.13728696]\n",
      " [0.92244084 0.12971537 0.08925453]\n",
      " [0.95156408 0.09214712 0.06889302]\n",
      " [0.92909346 0.1198428  0.08554553]\n",
      " [0.8360482  0.21846568 0.13675099]\n",
      " [0.79793423 0.25405831 0.15381675]\n",
      " [0.88126868 0.1744464  0.11342322]\n",
      " [0.89841922 0.1554143  0.10426799]\n",
      " [0.92148338 0.12992273 0.09014595]\n",
      " [0.35017065 0.6080899  0.35418129]\n",
      " [0.34757701 0.60864419 0.35655417]\n",
      " [0.34485093 0.60798741 0.3597076 ]\n",
      " [0.33319076 0.58601195 0.38584756]\n",
      " [0.34307618 0.60929743 0.36073295]\n",
      " [0.34747875 0.6075668  0.35726418]\n",
      " [0.27277537 0.48018427 0.51605187]\n",
      " [0.34583352 0.60757867 0.35892922]\n",
      " [0.24888662 0.44147417 0.57345554]\n",
      " [0.34486588 0.6090622  0.35906087]\n",
      " [0.45426505 0.52922097 0.30003273]\n",
      " [0.34570196 0.60896628 0.35827162]\n",
      " [0.34481183 0.6075073  0.35998426]\n",
      " [0.27540735 0.47234957 0.51329246]\n",
      " [0.34434496 0.60854312 0.3598212 ]\n",
      " [0.34552802 0.60707988 0.35945398]\n",
      " [0.34638702 0.61184623 0.355849  ]\n",
      " [0.34485682 0.60909355 0.35906042]\n",
      " [0.345009   0.608208   0.35934367]\n",
      " [0.17789716 0.29790128 0.75024521]\n",
      " [0.34495917 0.6076517  0.35976248]\n",
      " [0.3487524  0.60596082 0.35683615]\n",
      " [0.56290995 0.43789901 0.25308301]\n",
      " [0.23962172 0.40771112 0.59751556]\n",
      " [0.26269869 0.46565226 0.54303782]\n",
      " [0.34735504 0.60709162 0.35767771]\n",
      " [0.34573384 0.60769954 0.35892842]\n",
      " [0.34540071 0.6069144  0.35970882]\n",
      " [0.24732871 0.42443515 0.57503519]\n",
      " [0.34389451 0.60821137 0.36048014]\n",
      " [0.21175087 0.36169017 0.66691967]\n",
      " [0.10837262 0.14639296 0.89859125]\n",
      " [0.07097497 0.07778488 0.95509861]\n",
      " [0.081458   0.09342862 0.94261868]\n",
      " [0.06018972 0.05729257 0.96800996]\n",
      " [0.08421217 0.09666164 0.93819531]\n",
      " [0.06364528 0.06279    0.96428178]\n",
      " [0.084267   0.09569489 0.93849032]\n",
      " [0.0552824  0.0496524  0.97272601]]\n",
      "[[1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "#the test data\n",
    "testX = array(inData[nSPerClass:class2start]+inData[class2start+nSPerClass:class3start]+inData[class3start+nSPerClass:]) #rows are the rest of the input data\n",
    "testY = array(outData[nSPerClass:class2start]+outData[class2start+nSPerClass:class3start]+outData[class3start+nSPerClass:]).astype(float) #rows are rest of output data\n",
    "testSY = array(outSData[nSPerClass:class2start]+outSData[class2start+nSPerClass:class3start]+outSData[class3start+nSPerClass:]).astype(float) #thing to check results with    \n",
    "\n",
    "testX = normalizeByTrain(testX)\n",
    "#now feed forward again but on test data with theoretically more accurate W\n",
    "pred=[]\n",
    "for i in arange(len(testX[:,0])): #i iterates over number of rows in training data\n",
    "    x=testX[i,:] #x grabs the i-th row\n",
    "    result=feed_forward(x,newW)\n",
    "    pred.append(result)\n",
    "\n",
    "print(array(pred))\n",
    "print((array(pred)+0.5).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-skill",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
