{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tested-meditation",
   "metadata": {},
   "source": [
    "Importing the data from the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "spectacular-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import csv\n",
    "from scipy.optimize import fmin_bfgs,minimize\n",
    "\n",
    "#data specific numbers\n",
    "class1start = 0\n",
    "class2start = 59\n",
    "class3start = 130\n",
    "\n",
    "#global parameters\n",
    "regParam = 0.03 #use in cost function\n",
    "nSPerClass = 40 #number of samples per class\n",
    "nS = 3*nSPerClass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-mirror",
   "metadata": {},
   "source": [
    "Stuff from Assignment \\#1 which we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "included-reason",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LReLU, optional to leave as ReLU by passing esp=0 \n",
    "def lrelu(x,b=1,esp=0.01):\n",
    "    y=[]\n",
    "    for xi in x:\n",
    "        if xi<0:\n",
    "            y.append(esp*xi)\n",
    "        else:\n",
    "            y.append(b*xi)\n",
    "    return array(y)\n",
    "def lrelu_prime(x,b=1,esp=0.01):\n",
    "    y=[]\n",
    "    for xi in x:\n",
    "        if xi<0:\n",
    "            y.append(esp)\n",
    "        else:\n",
    "            y.append(b)\n",
    "    return array(y)\n",
    "#x is input vector, W is weight estimation (use random entries between 0 and 1 for first estimate, call N somewhere)\n",
    "def feed_forward(x,W):\n",
    "    W0 = array(W[:k*m]).reshape((k,m))\n",
    "    b0 = W[k*m:k*m+k]\n",
    "    W1 = array(W[(m+1)*k:(m+1)*k+n*k]).reshape((n,k))\n",
    "    b1 = W[(m+1)*k+n*k:]\n",
    "    z1 = dot(W0,x)+b0\n",
    "    z2 = dot(W1,a1(z1))+b1\n",
    "    return a2(z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-fluid",
   "metadata": {},
   "source": [
    "Begin Assignment \\#2 here. Much is borrowed from your code but used my own terms as I wrote it out to make sure I understood. I am going to use the binary encoding rather than one-hot so I need the ReLU and Logistic activation functions, and I'm using an ensemble cost function rather than the other method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "coral-springfield",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------The rest of the functions we should need are in here----------------------------------\n",
    "#many of which have been hybrid-ed with Dr. COo\n",
    "#accepts a column vector argument, returns mean and l2 norm of vector\n",
    "def normalize(col): \n",
    "    l2norm = sqrt(sum((col-mean(col))*(col-mean(col)))/len(col))\n",
    "    return mean(col),l2norm\n",
    "\n",
    "#accepts no arguments, returns array of mean values for data & array of norm values\n",
    "def normalize_cols(inMat):\n",
    "    means=[]\n",
    "    norms=[]\n",
    "    for i in range(m):\n",
    "        mn,nrm=normalize(inMat[:,i])\n",
    "        means.append(mn)\n",
    "        norms.append(nrm)\n",
    "    meanValues=array(means)\n",
    "    normValues=array(norms)\n",
    "    return meanValues,normValues\n",
    "\n",
    "#accepts a matrix, returns the normalized matrix\n",
    "def normalizeByTrain(inMat):\n",
    "    meanVals,normVals = normalize_cols(inMat)\n",
    "    normedMat=inMat*0.0\n",
    "    for k in range(len(inMat[:,0])):\n",
    "        normedMat[k]=(inMat[k]-meanVals)/normVals\n",
    "    return normedMat\n",
    "\n",
    "#accepts input matrices X and Y and vector W\n",
    "def ensemble_cost(W,X,Y):\n",
    "    C = 0.0\n",
    "    for i in range(len(X[0,:])):\n",
    "        x=X[i,:]\n",
    "        y=Y[i,:]\n",
    "        C += 0.5*(dot(feed_forward(x,W)-y,feed_forward(x,W)-y)+regParam*0.5*linalg.norm(W)**2)\n",
    "    return C\n",
    "\n",
    "#Dr. Cooper's logistic function & it's derivative below + his other functions b/c I think they'll work more reliably\n",
    "def S(X):\n",
    "    X = X*(abs(X)<10)+10.*(X>=10)-10.*(X<=-10)\n",
    "    return 1./(1+exp(-X))\n",
    "\n",
    "def SPrime(X):\n",
    "    X = X*(abs(X)<10)+10.*(X>=10)-10.*(X<=-10)\n",
    "    ex = exp(-X)\n",
    "    return ex/((1.+ex)*(1.+ex))\n",
    "\n",
    "def relu(x,epsilon=0.0):\n",
    "    return x*(x>0)+epsilon*x*(x<=0)\n",
    "\n",
    "def reluPrime(x,epsilon=0.0):\n",
    "    return (x>0)+epsilon*(x<=0)\n",
    "\n",
    "def selu(x,lamda=1,alpha=1):\n",
    "    return lamda*( x*(x>0)+alpha*(exp(x)-1)*(x<=0))\n",
    "\n",
    "def seluPrime(x,lamda=1,alpha=1):\n",
    "    return lamda*((x>0)+alpha*exp(x)*(x<=0))\n",
    "\n",
    "#accepts input vector x, output vector out, and big W vector, returns a (regularized) gradient for one row\n",
    "def gradient(x,out,W):#<----borrowed\n",
    "    W0 = array(W[:nW0]).reshape((k,m))\n",
    "    W1 = array(W[nW0+k:-n]).reshape((n,k))\n",
    "    grad = W*0.0\n",
    "    # Get a(z_2)\n",
    "    z1 = dot(W0,x)+W[nW0:nW0+k]\n",
    "    z2 = dot(W1,a1(z1))+W[-n:]\n",
    "    forward = a2(z2)\n",
    "    err = forward-out\n",
    "    # gradient for b_1 bias weights\n",
    "    finalLayerDeriv = a2p(z2)*err\n",
    "    grad[-n:] = finalLayerDeriv+0.\n",
    "    # gradient for W_1 weights\n",
    "    grad[(nW0+k):-n] = outer(finalLayerDeriv,a1(z1)).flatten()\n",
    "    # gradient for b_0 bias weights\n",
    "    firstLayerDeriv = finalLayerDeriv.dot(W1)*a1p(z1)\n",
    "    grad[nW0:(nW0+k)] = firstLayerDeriv+0.\n",
    "    # gradient for W_0 weights\n",
    "    grad[:nW0] = outer(firstLayerDeriv,x).flatten()\n",
    "    return grad+regParam*W\n",
    "\n",
    "#accepts vector W, input matrix X and output matrix Y\n",
    "def gradSum(W,X,Y):\n",
    "    grad = zeros(nVar)\n",
    "    for i in range(len(X[:,0])): #X[:,0] is a slice of all the rows in the first column\n",
    "        grad += gradient(X[i,:],Y[i,:],W)\n",
    "    return grad\n",
    "def int2bits(n,bits=4):\n",
    "    return [int(b) for b in list(format(n, \"0\"+str(bits)+\"b\"))]\n",
    "def bits2int(lst):\n",
    "    integer =0\n",
    "    for i in range(len(lst)):\n",
    "        integer += lst[i]*2**(len(lst)-i-1)\n",
    "    return integer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-cable",
   "metadata": {},
   "source": [
    "The code which will actually produce the neural network structure now is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lightweight-bacteria",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'uciWineLocation.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b9b782f92685>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"uciWineLocation.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0minData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moutData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#will contain truth values for each of the three classes, i.e. 1 == [true false false], used for one-hot?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moutSData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#will contain the numerical values corresponding to each class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;31m#n is the number of outputs desired\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'uciWineLocation.csv'"
     ]
    }
   ],
   "source": [
    "file = csv.reader(open(\"uciWineLocation.csv\",\"r\"))\n",
    "inData = []\n",
    "outData = [] #will contain truth values for each of the three classes, i.e. 1 == [true false false], used for one-hot?\n",
    "outSData = [] #will contain the numerical values corresponding to each class\n",
    "n = 2 #n is the number of outputs desired\n",
    "testBin = n*[0]\n",
    "\n",
    "for row in file:\n",
    "    for ri in range(len(row)):\n",
    "        row[ri] = float(row[ri])\n",
    "    inData.append(row[1:])\n",
    "#     for i in range(1,4): #this loop writes it as one-hot\n",
    "#         testBin[i-1] = (row[0]==float(i))\n",
    "    testBin = int2bits(int(row[0]),2)\n",
    "    outData.append(testBin.copy())#takes the encoded outdata to actually run through the neural network\n",
    "    outSData.append(row[0]) #what to compare to later, the numbers 1,2,3\n",
    "#Peeling off the training data into vectors\n",
    "trainX = array(inData[:nSPerClass]+inData[class2start:class2start+nSPerClass]+inData[class3start:class3start+nSPerClass]) #rows are input data\n",
    "trainY = array(outData[:nSPerClass]+outData[class2start:class2start+nSPerClass]+outData[class3start:class3start+nSPerClass]).astype(float) #rows are output data\n",
    "trainSY = array(outSData[:nSPerClass]+outSData[class2start:class2start+nSPerClass]+outSData[class3start:class3start+nSPerClass]).astype(float) #thing to check results with\n",
    "m = len(trainX[0]) #m is the number of inputs, i.e. number of columns in first row\n",
    "k = int(2*m/3+n) #k is the number of hidden layers\n",
    "W = random.rand((m + 1)*k+(k + 1)*n) #our first estimate of our W\n",
    "print('Number of samples: {}'.format(len(inData)))\n",
    "print('Number of data vectors: {}'.format(nS))\n",
    "print('Number of data items: {}'.format(m))\n",
    "print('Number of hidden cells: {}'.format(k))\n",
    "print(trainY)\n",
    "nW0 = m*k\n",
    "nW1 = k*n\n",
    "nVar = nW0+nW1+k+n\n",
    "\n",
    "#normalize the vector\n",
    "trainX = normalizeByTrain(trainX)\n",
    "#compute cost, choosing a1 to be relu and a2 to be logistic\n",
    "a1 = lambda x: lrelu(x)\n",
    "a1p = lambda x: lrelu_prime(x)\n",
    "a2 = S\n",
    "a2p = SPrime\n",
    "#call training function, I'm going to use bfgs_min to solve just because it seems best\n",
    "newW = W\n",
    "for i in range(3):\n",
    "    newW = fmin_bfgs(ensemble_cost,newW,fprime=gradSum,args=(trainX,trainY))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "virtual-halloween",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]]\n"
     ]
    }
   ],
   "source": [
    "#the test data\n",
    "testX = array(inData[nSPerClass:class2start]+inData[class2start+nSPerClass:class3start]+inData[class3start+nSPerClass:]) #rows are the rest of the input data\n",
    "testY = array(outData[nSPerClass:class2start]+outData[class2start+nSPerClass:class3start]+outData[class3start+nSPerClass:]).astype(float) #rows are rest of output data\n",
    "testSY = array(outSData[nSPerClass:class2start]+outSData[class2start+nSPerClass:class3start]+outSData[class3start+nSPerClass:]).astype(float) #thing to check results with    \n",
    "testX = normalizeByTrain(testX)\n",
    "#now feed forward again but on test data with theoretically more accurate W\n",
    "pred=[]\n",
    "for i in arange(len(testX[:,0])): #i iterates over number of rows in training data\n",
    "    x=testX[i,:] #x grabs the i-th row\n",
    "    result=feed_forward(x,newW)\n",
    "    pred.append(result)\n",
    "\n",
    "print((array(pred)+0.5).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-appearance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
