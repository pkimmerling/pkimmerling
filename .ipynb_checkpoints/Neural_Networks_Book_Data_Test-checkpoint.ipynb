{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tested-meditation",
   "metadata": {},
   "source": [
    "This file will test whether or not the books are paperbacks or hardbacks. It doesn't seem to be a good data set.\n",
    "\n",
    "Importing the data from the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "spectacular-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import csv\n",
    "from scipy.optimize import fmin_bfgs,minimize\n",
    "\n",
    "#data specific numbers\n",
    "class1start = 0\n",
    "class2start = 34\n",
    "\n",
    "#global parameters\n",
    "regParam = 0.03 #use in cost function\n",
    "nSPerClass = 30 #number of samples per class\n",
    "nS = 2*nSPerClass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-mirror",
   "metadata": {},
   "source": [
    "Stuff from Assignment \\#1 which we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "included-reason",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LReLU, optional to leave as ReLU by passing esp=0 \n",
    "def lrelu(x,b=1,esp=0.01):\n",
    "    y=[]\n",
    "    for xi in x:\n",
    "        if xi<0:\n",
    "            y.append(esp*xi)\n",
    "        else:\n",
    "            y.append(b*xi)\n",
    "    return array(y)\n",
    "def lrelu_prime(x,b=1,esp=0.01):\n",
    "    y=[]\n",
    "    for xi in x:\n",
    "        if xi<0:\n",
    "            y.append(esp)\n",
    "        else:\n",
    "            y.append(b)\n",
    "    return array(y)\n",
    "#x is input vector, W is weight estimation (use random entries between 0 and 1 for first estimate, call N somewhere)\n",
    "def feed_forward(x,W):\n",
    "    W0 = array(W[:k*m]).reshape((k,m))\n",
    "    b0 = W[k*m:k*m+k]\n",
    "    W1 = array(W[(m+1)*k:(m+1)*k+n*k]).reshape((n,k))\n",
    "    b1 = W[(m+1)*k+n*k:]\n",
    "    z1 = dot(W0,x)+b0\n",
    "    z2 = dot(W1,a1(z1))+b1\n",
    "    return a2(z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-fluid",
   "metadata": {},
   "source": [
    "Begin Assignment \\#2 here. Much is borrowed from your code but used my own terms as I wrote it out to make sure I understood. I am going to use the binary encoding rather than one-hot so I need the ReLU and Logistic activation functions, and I'm using an ensemble cost function rather than the other method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "coral-springfield",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------The rest of the functions we should need are in here----------------------------------\n",
    "#many of which have been hybrid-ed with Dr. COo\n",
    "#accepts a column vector argument, returns mean and l2 norm of vector\n",
    "def normalize(col): \n",
    "    l2norm = sqrt(sum((col-mean(col))*(col-mean(col)))/len(col))\n",
    "    return mean(col),l2norm\n",
    "\n",
    "#accepts no arguments, returns array of mean values for data & array of norm values\n",
    "def normalize_cols(inMat):\n",
    "    means=[]\n",
    "    norms=[]\n",
    "    for i in range(m):\n",
    "        mn,nrm=normalize(inMat[:,i])\n",
    "        means.append(mn)\n",
    "        norms.append(nrm)\n",
    "    meanValues=array(means)\n",
    "    normValues=array(norms)\n",
    "    return meanValues,normValues\n",
    "\n",
    "#accepts a matrix, returns the normalized matrix\n",
    "def normalizeByTrain(inMat):\n",
    "    meanVals,normVals = normalize_cols(inMat)\n",
    "    normedMat=inMat*0.0\n",
    "    for k in range(len(inMat[:,0])):\n",
    "        normedMat[k]=(inMat[k]-meanVals)/normVals\n",
    "    return normedMat\n",
    "\n",
    "#accepts input matrices X and Y and vector W\n",
    "def ensemble_cost(W,X,Y):\n",
    "    C = 0.0\n",
    "    for i in range(len(X[0,:])):\n",
    "        x=X[i,:]\n",
    "        y=Y[i]\n",
    "        C += 0.5*(dot(feed_forward(x,W)-y,feed_forward(x,W)-y)+regParam*0.5*linalg.norm(W)**2)\n",
    "    return C\n",
    "\n",
    "#Dr. Cooper's logistic function & it's derivative below + his other functions b/c I think they'll work more reliably\n",
    "def S(X):\n",
    "    X = X*(abs(X)<10)+10.*(X>=10)-10.*(X<=-10)\n",
    "    return 1./(1+exp(-X))\n",
    "\n",
    "def SPrime(X):\n",
    "    X = X*(abs(X)<10)+10.*(X>=10)-10.*(X<=-10)\n",
    "    ex = exp(-X)\n",
    "    return ex/((1.+ex)*(1.+ex))\n",
    "\n",
    "def relu(x,epsilon=0.0):\n",
    "    return x*(x>0)+epsilon*x*(x<=0)\n",
    "\n",
    "def reluPrime(x,epsilon=0.0):\n",
    "    return (x>0)+epsilon*(x<=0)\n",
    "\n",
    "def selu(x,lamda=1,alpha=1):\n",
    "    return lamda*( x*(x>0)+alpha*(exp(x)-1)*(x<=0))\n",
    "\n",
    "def seluPrime(x,lamda=1,alpha=1):\n",
    "    return lamda*((x>0)+alpha*exp(x)*(x<=0))\n",
    "\n",
    "#accepts input vector x, output vector out, and big W vector, returns a (regularized) gradient for one row\n",
    "def gradient(x,out,W):#<----borrowed\n",
    "    W0 = array(W[:nW0]).reshape((k,m))\n",
    "    W1 = array(W[nW0+k:-n]).reshape((n,k))\n",
    "    grad = W*0.0\n",
    "    # Get a(z_2)\n",
    "    z1 = dot(W0,x)+W[nW0:nW0+k]\n",
    "    z2 = dot(W1,a1(z1))+W[-n:]\n",
    "    forward = a2(z2)\n",
    "    err = forward-out\n",
    "    # gradient for b_1 bias weights\n",
    "    finalLayerDeriv = a2p(z2)*err\n",
    "    grad[-n:] = finalLayerDeriv+0.\n",
    "    # gradient for W_1 weights\n",
    "    grad[(nW0+k):-n] = outer(finalLayerDeriv,a1(z1)).flatten()\n",
    "    # gradient for b_0 bias weights\n",
    "    firstLayerDeriv = finalLayerDeriv.dot(W1)*a1p(z1)\n",
    "    grad[nW0:(nW0+k)] = firstLayerDeriv+0.\n",
    "    # gradient for W_0 weights\n",
    "    grad[:nW0] = outer(firstLayerDeriv,x).flatten()\n",
    "    return grad+regParam*W\n",
    "\n",
    "#accepts vector W, input matrix X and output matrix Y\n",
    "def gradSum(W,X,Y):\n",
    "    grad = zeros(nVar)\n",
    "    for i in range(len(X[:,0])): #X[:,0] is a slice of all the rows in the first column\n",
    "        grad += gradient(X[i,:],Y[i],W)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-cable",
   "metadata": {},
   "source": [
    "The code which will actually produce the neural network structure now is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "lightweight-bacteria",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 98\n",
      "Number of data vectors: 60\n",
      "Number of data items: 6\n",
      "Number of hidden cells: 5\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.809023\n",
      "         Iterations: 4\n",
      "         Function evaluations: 72\n",
      "         Gradient evaluations: 60\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.675475\n",
      "         Iterations: 1\n",
      "         Function evaluations: 63\n",
      "         Gradient evaluations: 51\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.675475\n",
      "         Iterations: 0\n",
      "         Function evaluations: 73\n",
      "         Gradient evaluations: 61\n"
     ]
    }
   ],
   "source": [
    "file = csv.reader(open(\"BooksDataNN.csv\",\"r\"))\n",
    "inData = []\n",
    "outData = [] #encoded output data, whether binary or one-hot\n",
    "outSData = [] #will contain the numerical values corresponding to each class\n",
    "n = 1 #n is the number of outputs desired\n",
    "testBin = n*[0]\n",
    "\n",
    "for row in file:\n",
    "    for ri in range(1,7):\n",
    "        row[ri] = float(row[ri])\n",
    "    inData.append(row[1:7])\n",
    "#     for i in range(1,4): #this loop writes it as one-hot\n",
    "#         testBin[i-1] = (row[0]==float(i))\n",
    "    if row[0] == \"hard\": #ifelse chain writes it as binary\n",
    "        testBin = 0\n",
    "    else:\n",
    "        testBin = 1\n",
    "    outData.append(testBin)#takes the encoded outdata to actually run through the neural network\n",
    "    outSData.append(row[0]) #what to compare to later\n",
    "#Peeling off the training data into vectors\n",
    "trainX = array(inData[:nSPerClass]+inData[class2start:class2start+nSPerClass]) #rows are input data\n",
    "trainY = array(outData[:nSPerClass]+outData[class2start:class2start+nSPerClass]).astype(float) #rows are output data\n",
    "trainSY = array(outSData[:nSPerClass]+outSData[class2start:class2start+nSPerClass]) #thing to check results with\n",
    "m = len(trainX[0]) #m is the number of inputs, i.e. number of columns in first row\n",
    "k = int(2*m/3+n) #k is the number of hidden layers\n",
    "W = random.rand((m + 1)*k+(k + 1)*n) #our first estimate of our W\n",
    "print('Number of samples: {}'.format(len(inData)))\n",
    "print('Number of data vectors: {}'.format(nS))\n",
    "print('Number of data items: {}'.format(m))\n",
    "print('Number of hidden cells: {}'.format(k))\n",
    "print(trainY)\n",
    "nW0 = m*k\n",
    "nW1 = k*n\n",
    "nVar = nW0+nW1+k+n\n",
    "\n",
    "#normalize the vector\n",
    "trainX = normalizeByTrain(trainX)\n",
    "#compute cost, choosing a1 to be relu and a2 to be logistic\n",
    "a1 = lambda x: lrelu(x)\n",
    "a1p = lambda x: lrelu_prime(x)\n",
    "a2 = S\n",
    "a2p = SPrime\n",
    "#call training function, I'm going to use bfgs_min to solve just because it seems best\n",
    "newW = W\n",
    "for i in range(3):\n",
    "    newW = fmin_bfgs(ensemble_cost,newW,fprime=gradSum,args=(trainX,trainY))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "virtual-halloween",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.47103462]\n",
      " [0.47080977]\n",
      " [0.47100033]\n",
      " [0.47189359]\n",
      " [0.47607054]\n",
      " [0.47601493]\n",
      " [0.47624334]\n",
      " [0.47594077]\n",
      " [0.47613081]\n",
      " [0.47413066]\n",
      " [0.4738384 ]\n",
      " [0.47582878]\n",
      " [0.47591464]\n",
      " [0.47623656]\n",
      " [0.47216288]\n",
      " [0.4728799 ]\n",
      " [0.47540478]\n",
      " [0.47584154]\n",
      " [0.4759412 ]\n",
      " [0.46676327]\n",
      " [0.4759273 ]\n",
      " [0.47462421]\n",
      " [0.47567839]\n",
      " [0.47632917]\n",
      " [0.47646492]\n",
      " [0.47624211]\n",
      " [0.47631888]\n",
      " [0.47596022]\n",
      " [0.47579835]\n",
      " [0.47243534]\n",
      " [0.46931608]\n",
      " [0.46335975]\n",
      " [0.47599666]\n",
      " [0.4761889 ]\n",
      " [0.47619346]\n",
      " [0.47603913]\n",
      " [0.47613345]\n",
      " [0.47531659]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "#the test data\n",
    "testX = array(inData[nSPerClass:class2start]+inData[class2start+nSPerClass:]) #rows are the rest of the input data\n",
    "testY = array(outData[nSPerClass:class2start]+outData[class2start+nSPerClass:]).astype(float) #rows are rest of output data\n",
    "testSY = array(outSData[nSPerClass:class2start]+outSData[class2start+nSPerClass:]) #thing to check results with    \n",
    "testX = normalizeByTrain(testX)\n",
    "#now feed forward again but on test data with theoretically more accurate W\n",
    "pred=[]\n",
    "for i in arange(len(testX[:,0])): #i iterates over number of rows in training data\n",
    "    x=testX[i,:] #x grabs the i-th row\n",
    "    result=feed_forward(x,newW)\n",
    "    pred.append(result)\n",
    "print(array(pred))\n",
    "print((array(pred)+0.5).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "swiss-snowboard",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.45972977  0.14782286  1.41036966 -0.55279364  0.33562143  0.49564469]\n",
      " [ 0.32164645 -1.67637418  0.60420971 -0.31010375  0.66922104  0.77914023]\n",
      " [ 0.41698969  0.4623396   2.36422305  2.52127829 -0.02830542  0.04205184]\n",
      " [ 0.28383793  0.39943625  1.58267866  2.03589851  0.03234905  0.07040139]\n",
      " [ 0.54849761 -2.30540764  1.8103727   0.66065581 -0.05863266  0.04205184]\n",
      " [-0.17150824 -2.30540764 -1.29118927 -0.55279364  0.45693038  0.72244112]\n",
      " [ 0.37424962 -1.67637418  0.0995905   0.33706929  0.8815117   0.58069335]\n",
      " [ 2.25481287 -0.29250056  1.13959838  1.55051873  1.57903816  1.28943219]\n",
      " [ 1.29480506 -1.23605075 -0.31887421  0.0943794   2.73147318  2.05487014]\n",
      " [ 0.70630712 -2.1166976  -1.33426652 -0.71458689  2.73147318  3.01875496]\n",
      " [ 3.65865991  0.65104963  0.14882164  0.74155244  2.64049147  2.76360898]\n",
      " [ 3.79345552  0.71395298  0.17343721  1.06513896  2.70114594  2.76360898]\n",
      " [ 3.36441094  0.77685633 -0.02348736  0.66065581  2.67081871  2.76360898]\n",
      " [-0.11397353  0.52524294  0.23497614  0.33706929  0.03234905 -0.07134638]\n",
      " [-0.26356379  0.4623396  -0.5834916  -0.14831049  0.03234905 -0.07134638]\n",
      " [-0.04328802  0.52524294  0.05651325  0.25617266  0.09300353 -0.09969593]\n",
      " [-0.71890996  0.02201617 -2.02965641 -1.60444982 -0.14961437 -0.35484191]\n",
      " [-0.71397841  0.02201617 -1.94350191 -1.60444982 -0.14961437 -0.35484191]\n",
      " [ 0.93808982  0.39943625  2.38268473  2.19769177  0.3962759   0.60904291]\n",
      " [ 0.71945791  0.27362956  1.7242182   1.4696221   0.3962759   0.63739246]\n",
      " [-0.11068583  0.96556637 -0.52195267  0.01348277  0.18398524  0.09875094]\n",
      " [-0.12383662  0.58814629  0.11189828  0.49886255  0.12333077 -0.07134638]\n",
      " [-0.12712432  0.52524294  0.11805218  0.49886255  0.12333077 -0.07134638]\n",
      " [ 0.04054828  0.77685633  0.9488277   1.79320862  0.06267629  0.07040139]\n",
      " [ 0.11452148  0.71395298  0.94267381  1.71231199  0.03234905  0.09875094]\n",
      " [-0.49370265  1.1542764  -1.4388827  -1.11907004  0.36594866 -1.57387271]\n",
      " [-0.26027609  1.2800831  -0.22041193  0.25617266  0.06267629  0.1271005 ]\n",
      " [-0.15835745  1.1542764   0.21036057  0.49886255  0.09300353  0.07040139]\n",
      " [-0.1698644   1.09137306  0.20420668  0.57975918  0.09300353  0.07040139]\n",
      " [-0.32603005  0.39943625  0.27805339  0.82244907 -0.30125056 -0.80843477]\n",
      " [-0.42959254 -0.29250056 -1.02657188 -1.19996667  0.18398524  0.46729514]\n",
      " [ 0.21150857  0.02201617  0.41343903 -0.14831049  0.153658    0.58069335]\n",
      " [-0.44931872  0.52524294 -0.91580181 -1.03817341 -0.17994161 -0.01464727]\n",
      " [-0.07287731  0.3365329   1.16421395  1.38872547 -0.21026885 -0.12804548]\n",
      " [-0.17808364  0.27362956  0.77651871  0.82244907 -0.21026885 -0.18474459]\n",
      " [-0.7337046   0.65104963 -0.65118442 -0.55279364 -1.24139492 -1.20532852]\n",
      " [-0.39835941  0.39943625  2.24729908  0.49886255 -1.24139492 -1.20532852]\n",
      " [-0.36219473  0.39943625  0.48113185 -0.06741386 -0.24059608 -0.01464727]\n",
      " [-0.72384151 -1.42476079 -0.97118685 -1.2808633  -1.18074044 -0.97853209]\n",
      " [-0.75671849 -1.92798756 -0.94657128 -1.2808633  -1.15041321 -0.97853209]\n",
      " [-0.75671849 -1.67637418 -0.94657128 -1.19996667 -1.18074044 -0.97853209]\n",
      " [-0.76658158 -1.36185745 -0.9281096  -1.19996667 -1.15041321 -1.00688164]\n",
      " [-0.72877305 -1.2989541  -0.64503053 -1.03817341 -1.15041321 -1.0352312 ]\n",
      " [-0.78466392 -1.67637418 -1.50042163 -1.52355319 -0.42255951 -0.63833745]\n",
      " [-0.55288121 -2.43121433 -0.98964853 -1.44265656 -0.17994161 -0.12804548]\n",
      " [-0.26356379  0.65104963  0.03189768 -0.14831049 -0.45288675 -0.2981428 ]\n",
      " [-0.33589314  0.83975967  0.59190192 -0.14831049 -0.21026885  0.01370228]\n",
      " [-0.46411337  0.39943625 -0.31272032 -0.31010375 -0.17994161 -0.12804548]\n",
      " [-0.36877012  0.14782286 -0.20195025 -0.31010375 -0.24059608 -0.32649236]\n",
      " [-0.38027707  0.71395298  0.77651871  0.57975918 -0.54386846 -0.49658968]\n",
      " [-0.5693197   0.52524294 -0.85426288 -0.95727678 -0.36190503 -0.2981428 ]\n",
      " [-0.61041593  0.08491952 -0.40502871 -1.03817341 -0.51354122 -0.49658968]\n",
      " [-0.61863517  0.02201617 -0.62041496 -0.87638015 -0.45288675 -0.35484191]\n",
      " [-0.61205977  0.4623396  -0.78657006 -0.87638015 -0.21026885 -0.07134638]\n",
      " [-0.45918182  0.96556637 -0.21425803 -0.39100038 -0.21026885 -0.18474459]\n",
      " [-0.67123834  0.39943625 -0.10348796 -0.06741386 -1.18074044 -1.12027986]\n",
      " [-0.67123834  0.39943625 -0.09733407 -0.14831049 -1.21106768 -1.12027986]\n",
      " [-0.66301909  0.58814629 -0.10348796  0.01348277 -1.21106768 -1.12027986]\n",
      " [-0.6564437   0.39943625 -0.00502568  0.01348277 -1.24139492 -1.12027986]\n",
      " [-0.66795064  0.4623396  -0.09733407 -0.06741386 -1.21106768 -1.12027986]]\n"
     ]
    }
   ],
   "source": [
    "print(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-seventh",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
